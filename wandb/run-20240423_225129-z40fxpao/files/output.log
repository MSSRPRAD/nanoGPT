Traceback (most recent call last):
  File "/home/mpradyumna/Documents/GitHub/nanoGPT/train.py", line 264, in <module>
    losses = estimate_loss()
             ^^^^^^^^^^^^^^^
  File "/home/mpradyumna/Documents/GitHub/nanoGPT/venv/lib64/python3.12/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/mpradyumna/Documents/GitHub/nanoGPT/train.py", line 224, in estimate_loss
    logits, loss = model(X, Y)
                   ^^^^^^^^^^^
  File "/home/mpradyumna/Documents/GitHub/nanoGPT/venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mpradyumna/Documents/GitHub/nanoGPT/venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mpradyumna/Documents/GitHub/nanoGPT/model.py", line 216, in forward
    tok_emb = self.transformer.wte(idx.float()) # token embeddings of shape (b, t, n_embd)
                                   ^^^^^^^^^^^
KeyboardInterrupt