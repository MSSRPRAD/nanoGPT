
step 0: train loss 10.8833, val loss 10.9081
iter 0: loss 10.8897, time 57544.32ms, mfu -100.00%
iter 10: loss 10.6941, time 149.98ms, mfu 8.26%
iter 20: loss 9.8753, time 149.50ms, mfu 8.26%
iter 30: loss 9.0537, time 149.51ms, mfu 8.26%
iter 40: loss 7.8402, time 149.42ms, mfu 8.26%
iter 50: loss 7.2809, time 149.35ms, mfu 8.27%
iter 60: loss 6.3678, time 149.98ms, mfu 8.27%
iter 70: loss 5.9580, time 149.44ms, mfu 8.27%
iter 80: loss 6.8371, time 149.75ms, mfu 8.27%
iter 90: loss 6.4474, time 149.43ms, mfu 8.27%
iter 100: loss 5.7723, time 149.31ms, mfu 8.27%
iter 110: loss 6.1911, time 149.04ms, mfu 8.28%
iter 120: loss 4.8789, time 149.06ms, mfu 8.28%
iter 130: loss 4.6253, time 149.33ms, mfu 8.28%
iter 140: loss 5.4068, time 149.48ms, mfu 8.28%
iter 150: loss 5.9660, time 149.11ms, mfu 8.28%
iter 160: loss 5.7521, time 149.58ms, mfu 8.28%
iter 170: loss 4.5084, time 149.94ms, mfu 8.28%
iter 180: loss 5.1886, time 149.12ms, mfu 8.28%
iter 190: loss 5.1236, time 149.52ms, mfu 8.28%
iter 200: loss 4.6937, time 149.20ms, mfu 8.28%
iter 210: loss 4.3368, time 149.49ms, mfu 8.28%
iter 220: loss 4.6737, time 149.43ms, mfu 8.28%
iter 230: loss 5.0363, time 148.34ms, mfu 8.29%
iter 240: loss 5.3860, time 149.55ms, mfu 8.29%
step 250: train loss 4.7646, val loss 6.7005
saving checkpoint to out-circuits
iter 250: loss 4.9440, time 25460.19ms, mfu 7.46%
iter 260: loss 4.2580, time 150.05ms, mfu 7.54%
iter 270: loss 4.7318, time 149.22ms, mfu 7.62%
iter 280: loss 5.0864, time 149.64ms, mfu 7.68%
iter 290: loss 4.7433, time 149.46ms, mfu 7.74%
iter 300: loss 5.6285, time 149.68ms, mfu 7.80%
iter 310: loss 5.0927, time 149.32ms, mfu 7.85%
iter 320: loss 5.2127, time 149.34ms, mfu 7.89%
iter 330: loss 4.7343, time 148.31ms, mfu 7.94%
iter 340: loss 4.4771, time 149.69ms, mfu 7.97%
iter 350: loss 5.2136, time 169.45ms, mfu 7.90%
iter 360: loss 5.0292, time 149.66ms, mfu 7.94%
iter 370: loss 4.5358, time 165.88ms, mfu 7.89%
iter 380: loss 4.2445, time 200.89ms, mfu 7.72%
iter 390: loss 5.0571, time 189.48ms, mfu 7.60%
iter 400: loss 5.2414, time 190.41ms, mfu 7.49%
iter 410: loss 5.2271, time 203.87ms, mfu 7.35%
iter 420: loss 4.6470, time 216.80ms, mfu 7.19%
iter 430: loss 5.2535, time 193.34ms, mfu 7.11%
iter 440: loss 4.3773, time 205.12ms, mfu 7.00%
iter 450: loss 4.8835, time 216.32ms, mfu 6.87%
iter 460: loss 4.5581, time 195.00ms, mfu 6.82%
iter 470: loss 4.3569, time 149.57ms, mfu 6.97%
iter 480: loss 4.8453, time 149.56ms, mfu 7.10%
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/nanoGPT/train.py", line 305, in <module>
    scaler.scale(loss).backward()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt